{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0d962d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "####  Top Skills of DS ( Data Scientists) on GlassDoor and Indeed\n",
    "This program aims at presenting the top 10 skills of DS listed in job descriptions of glassdoor and indeed. For the detail background, deliverables and processes, please see the readme in this git."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26a90261",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T09:47:02.699991Z",
     "start_time": "2022-05-18T09:47:02.695103Z"
    }
   },
   "outputs": [],
   "source": [
    "### Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f31b039",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T09:47:14.829509Z",
     "start_time": "2022-05-18T09:47:02.706689Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text preprocessing\n",
    "import os,re\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "\n",
    "# Disable warning of 3 types\n",
    "import warnings\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Other utils\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# Azure text analytics service api\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "\n",
    "# aws comprehend\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "#EDA tools.\n",
    "import dtale\n",
    "\n",
    "# Geopy for location\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# nlp text cleaning\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # or LancasterStemmer, RegexpStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba47d7e",
   "metadata": {},
   "source": [
    "### Pre-settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f079213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T09:47:14.845726Z",
     "start_time": "2022-05-18T09:47:14.834407Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the width to show the column as much as possible.\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "# Disable 3 types of warning\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=(FutureWarning))\n",
    "warnings.filterwarnings(\"ignore\",category=(RuntimeWarning))\n",
    "\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9415bf",
   "metadata": {},
   "source": [
    "### data cleaning\n",
    "    - Select the jobs of data related, and keep the data scientists' record for analysis.\n",
    "    - Remove the duplicated records.\n",
    "    - Convert job description in HTML to text.\n",
    "    - Store the cleaned data into main table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c92f1c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T10:00:00.141651Z",
     "start_time": "2022-05-18T09:47:14.852523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The processed data exist, do you want to reload it?(y/n)y\n",
      "Shape of source file: (165290, 163)\n",
      "Shape of jobs related to data: (7347, 10)\n"
     ]
    }
   ],
   "source": [
    "outputfile= './01_data/output/datajobs.csv'\n",
    "datafile='./01_data/input/glassdoor/glassdoor.csv'\n",
    "\n",
    "if os.path.exists(datafile):\n",
    "    if os.path.exists(outputfile):\n",
    "        Reload=input(\"The processed data exist, do you want to reload it?(y/n)\")\n",
    "    else:\n",
    "        Reload='y'\n",
    "        \n",
    "    # reload the data file, and re-produce the csv of data scientist\n",
    "    if Reload.lower()=='y':\n",
    "        try:\n",
    "            glassdoor=pd.read_csv(datafile)\n",
    "            print(\"Shape of source file:\", glassdoor.shape)\n",
    "             # Produce the list of jobs related to data\n",
    "            # Only keep the non-duplicated records by employer names and jobs.\n",
    "            # Only keep the first records if there are duplicated. Here I keep the latest one\n",
    "            # Sort the jobs by posted date ascendingly\n",
    "            data_jobs=glassdoor[glassdoor['header.jobTitle'].str.contains(' data ',case=False)].sort_values(\n",
    "                by='header.posted',ascending=False).loc[:,[\n",
    "                'gaTrackerData.industry',\n",
    "                'header.employerName',\n",
    "                'gaTrackerData.jobTitle',\n",
    "                'job.jobReqId.long',\n",
    "                'job.description',\n",
    "                'header.posted',\n",
    "                'map.country',\n",
    "                'map.lat',\n",
    "                'map.lng',\n",
    "                'map.location']]\n",
    "            # Keep the first record if the duplicated exist.\n",
    "            data_jobs['duplicated']=data_jobs.duplicated()\n",
    "            data_jobs_unique=data_jobs[data_jobs['duplicated']==False].loc[:,[\n",
    "                'gaTrackerData.industry',\n",
    "                'header.employerName',\n",
    "                'gaTrackerData.jobTitle',\n",
    "                'job.jobReqId.long',\n",
    "                'job.description',\n",
    "                'header.posted',\n",
    "                'map.country',\n",
    "                'map.lat',\n",
    "                'map.lng',\n",
    "                'map.location']]\n",
    "            data_jobs_unique.to_csv('./01_Data/Output/datajobs.csv')\n",
    "            print(\"Shape of jobs related to data:\", data_jobs_unique.shape)\n",
    "            # Assign id into each posted position for the coming identification\n",
    "            # Remove all html tag, and convert each requirements into one item for every posted position.\n",
    "            jobs = pd.DataFrame(\n",
    "                columns=[\n",
    "                    'posting_date',\n",
    "                    'description',\n",
    "                    'title',\n",
    "                    'country',\n",
    "                    'employer',\n",
    "                    'industry',\n",
    "                    'id',\n",
    "                    'source',\n",
    "                    'lat',\n",
    "                    'lng',\n",
    "                    'location']\n",
    "            )\n",
    "            #for i in tqdm(range(len(data_jobs_unique))):\n",
    "            \n",
    "            for i in range(len(data_jobs_unique)):\n",
    "                \n",
    "                html_page=data_jobs_unique.iloc[i,4]\n",
    "                soup = BeautifulSoup(html_page, 'html.parser')\n",
    "                jobs_list = soup.find_all(\"li\")\n",
    "                job_text=''\n",
    "                for job in jobs_list:\n",
    "                    try:\n",
    "                        lang = detect(str(job.contents[0]))\n",
    "                    except:\n",
    "                        lang = \"error\"\n",
    "                # Only handle the position described in English \n",
    "                # since this program is solely focusing on English \n",
    "                    if lang=='en':\n",
    "                        job_text=job_text + str(job.contents[0]).lower().split(\"\\r\\n\")[0]+'.'\n",
    " \n",
    "                # Create df to store the converted job description in text format.\n",
    "                if job_text!='':\n",
    "                    \n",
    "                    jobs=jobs.append(\n",
    "                        {\n",
    "                            \"posting_date\":data_jobs_unique.iloc[i,5],\n",
    "                            \"description\":job_text,\n",
    "                            \"title\":data_jobs_unique.iloc[i,2],\n",
    "                            \"country\":data_jobs_unique.iloc[i,6],\n",
    "                            \"employer\":data_jobs_unique.iloc[i,1],\n",
    "                            \"industry\":data_jobs_unique.iloc[i,0],\n",
    "                            \"id\":data_jobs_unique.iloc[i,3],\n",
    "                            \"source\":\"Glassdoor\",\n",
    "                            \"lat\":data_jobs_unique.iloc[i,7],\n",
    "                            \"lng\":data_jobs_unique.iloc[i,8],\n",
    "                            \"location\":data_jobs_unique.iloc[i,9]\n",
    "                        },\n",
    "                                      ignore_index=True) \n",
    "            # Prevent the issue of 'utf-8' encoding.    \n",
    "            jobs['description'] = jobs['description'].apply(lambda x: \n",
    "                                                            x.encode('ascii', 'ignore').decode('ascii'))\n",
    "            jobs.to_csv(outputfile)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to read the data file due to error:%s, please check the file or path!\" %e)\n",
    "    else:\n",
    "        jobs=pd.read_csv(outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a011b813",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:37:56.120256Z",
     "start_time": "2022-05-18T12:37:56.067292Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select data scientist jobs\n",
    "df_main=jobs[jobs['title'].str.contains(r'^(?=.*data)(?=.*scientist)',case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df85bed",
   "metadata": {},
   "source": [
    "### Fill / Standardize the country names\n",
    "\n",
    "- Those job postings without countries can find out countries by:\n",
    "    - From the job posting who has the same locations, but the country is NOT empty.\n",
    "    - Based on Location to look for the country names.\n",
    "\n",
    "- The short names of countries will be converted to full names based on the mapping of glassdoor's table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0f317b9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:37:57.947311Z",
     "start_time": "2022-05-18T12:37:57.936847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "372"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify how many jobs'country are empty\n",
    "len(df_main[df_main['country'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a87a6f6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:37:58.908147Z",
     "start_time": "2022-05-18T12:37:58.884479Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the mapping table for those which has country names and locations\n",
    "a_city=df_main[df_main['country'].isnull()==False]\\\n",
    "    [['country','location']].apply(lambda x: (x.iloc[0],x.iloc[1]),axis=1).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f2e6adc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:37:59.885320Z",
     "start_time": "2022-05-18T12:37:59.866909Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create replacing functions to \n",
    "# 1) return 1st element if 2nd element in array is equal to target string\n",
    "# 2) Return empty if target string could be not found\n",
    "def map_replace(a_source=[],s_target=''):\n",
    "    for item in a_source:\n",
    "        if pd.isna(item[1])!=True and pd.isna(s_target) != True:\n",
    "            if str(item[1]).strip().lower()==s_target.strip().lower():\n",
    "                return item[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c495ee8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:38:00.984569Z",
     "start_time": "2022-05-18T12:38:00.824312Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fill the country names\n",
    "df_main['country']=df_main.apply(lambda \n",
    "                                 x: map_replace(a_city,x.iloc[10]) if pd.isna(x.iloc[3]) else x.iloc[3],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "12201250",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:38:01.938537Z",
     "start_time": "2022-05-18T12:38:01.929422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many postings without country names still are left\n",
    "len(df_main[df_main['country'].isnull()==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0c62b6e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:38:08.273984Z",
     "start_time": "2022-05-18T12:38:08.249350Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import country mapping table for short names' conversion\n",
    "countryfile='./01_data/input/glassdoor/country_names_2_digit_codes.csv'\n",
    "df_country=pd.read_csv(countryfile)\n",
    "\n",
    "# Create function to get and standardize the country name\n",
    "def get_country(country='',lat='0',lng='0',city=''):\n",
    "    try:\n",
    "        country_name=''\n",
    "        \n",
    "        # country name's shortname to full name\n",
    "        if len(country)<=3 and len(country)>1:\n",
    "            country_name=df_country[df_country['Code'].str.lower()==country.lower()]['Name']\n",
    "            \n",
    "            if not country_name.empty:\n",
    "                \n",
    "                return country_name\n",
    "            \n",
    "        else:\n",
    "        # if country name does not exist, look for country name by geo location (latitude, longitude)\n",
    "            if country=='':\n",
    "                if (lat!='0' and lng!='0'):\n",
    "                    # initialize Nominatim API \n",
    "\n",
    "                    geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "\n",
    "                    # Latitude & Longitude input\n",
    "\n",
    "                    location = geolocator.reverse(lat+\",\"+lng,language='en')\n",
    "                    country_name = location.raw['address'].get('country', '')\n",
    "                    if country_name !='':\n",
    "                        return country_name\n",
    "        # if no geo location, search for country name by city name\n",
    "                else:\n",
    "                    if city !='':\n",
    "                        geolocator = Nominatim(timeout=10,user_agent=\"geoapiExercises\")\n",
    "                        #print(city)\n",
    "                        location = geolocator.geocode(city,language='en')\n",
    "                        loc_dict = location.raw\n",
    "                        #print(loc_dict)\n",
    "                        if loc_dict is not None:\n",
    "                            if ',' in loc_dict['display_name']:\n",
    "                                country_name=loc_dict['display_name'].rsplit(',' , 1)[1]\n",
    "                            else:\n",
    "                                country_name=loc_dict['display_name']\n",
    "                            return country_name\n",
    "            else:\n",
    "                return country\n",
    "                    \n",
    "    except Exception as e:\n",
    "            print(\"error:%s\" %e)\n",
    "            print(lat,lng,city,loc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1122346d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:40:50.262883Z",
     "start_time": "2022-05-18T12:38:09.327961Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fill the country names based on locations.\n",
    "df_main['country']=df_main.apply(lambda x: get_country(city=x.iloc[10])\n",
    "                                 if pd.isna(x.iloc[3]) else x.iloc[3],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "783c1ac7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:40:50.278873Z",
     "start_time": "2022-05-18T12:40:50.267736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many postings without country name are left.\n",
    "len(df_main[df_main['country'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e1f56a6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:40:50.310504Z",
     "start_time": "2022-05-18T12:40:50.286316Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add 2 Codes for USA and UK given previous mapping table is lack of them\n",
    "df_temp=pd.DataFrame({'Name':['United Kingdom','United States','Switzerland'],\n",
    "                      'Code':['UK','USA','CHE']},columns=['Name','Code']\n",
    "                              )\n",
    "df_country=df_country.append(df_temp,ignore_index=True)\n",
    "\n",
    "# Create mapping array for short names to full names\n",
    "a_name=df_country[['Name','Code']].apply(lambda x: (x.iloc[0],x.iloc[1]),axis=1).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "acf81984",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:40:50.328770Z",
     "start_time": "2022-05-18T12:40:50.316513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "383"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display numbers of records which has the short names of country\n",
    "len(df_main[(df_main['country'].str.len()<=3) & (df_main['country'].str.len()>=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "06320519",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:40:50.808358Z",
     "start_time": "2022-05-18T12:40:50.343389Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert short name of country to full name\n",
    "df_main['country']=df_main.apply(lambda x: map_replace(a_name,x.iloc[3]) \n",
    "                                 if (pd.isna(map_replace(a_name,x.iloc[3]))==False) else x.iloc[3],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5533d832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:40:50.826103Z",
     "start_time": "2022-05-18T12:40:50.811762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify what the short names are if these exist.\n",
    "len(df_main[df_main['country'].str.len()<=3][['country','location']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fca36994",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:40:50.900350Z",
     "start_time": "2022-05-18T12:40:50.830443Z"
    }
   },
   "outputs": [],
   "source": [
    "# Export main table only including the jobs of data scientists\n",
    "ds_file= './01_data/output/datascientists.csv'\n",
    "df_main.to_csv(ds_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "616d2c6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:45:06.968458Z",
     "start_time": "2022-05-18T12:45:06.657374Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform EDA to check main table\n",
    "d1 = dtale.show(df_main)\n",
    "d1.open_browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a38fa",
   "metadata": {},
   "source": [
    "### Fill / Standardize the industry name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e756a7cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:40:51.268812Z",
     "start_time": "2022-05-18T12:40:51.225736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify how many jobs'industry names are empty\n",
    "len(df_main[df_main['industry'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "91937ec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:40:51.329278Z",
     "start_time": "2022-05-18T12:40:51.283620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify whether the industry names could be found from those job postings with industry name and same employer\n",
    "len(set(df_main[df_main['industry'].isnull()==False]['employer']) &\\\n",
    "                set(df_main[df_main['industry'].isnull()]['employer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b2164450",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:40:51.369940Z",
     "start_time": "2022-05-18T12:40:51.336401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify whether the industry names could be found from those job postings with industry name and same employer\n",
    "len(set(df_main[df_main['industry'].isnull()==False]['employer']) &\\\n",
    "                set(jobs[jobs['industry'].isnull()]['employer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a967f121",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:40:51.407377Z",
     "start_time": "2022-05-18T12:40:51.375039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matched employers are found!\n"
     ]
    }
   ],
   "source": [
    "# Verify whether the industry names could be found from those job postings of indeed\n",
    "indeedfile='./01_data/input/employer_industry.csv'\n",
    "indeed=pd.read_csv(indeedfile)\n",
    "employer_num=len(set(df_main[df_main['industry'].isnull()]['employer']) &\\\n",
    "                set(indeed[indeed['industry'].isnull()==False]['employer']))\n",
    "\n",
    "#if the matched employers with industry are found, it will fill the industry of glassdoor jobs\n",
    "if employer_num>0:\n",
    "    print(\" %d employers are found in indeed file\" %employer_num)\n",
    "    # Create the mapping table for those which has industry names from indeed\n",
    "    a_industry=indeed[indeed['industry'].isnull()==False]\\\n",
    "        [['industry','employer']].apply(lambda x: (x.iloc[0],x.iloc[1]),axis=1).unique()\n",
    "    #Fill the industry names\n",
    "    df_main['industry']=df_main.apply(lambda \n",
    "                                     x: map_replace(a_industry,x.iloc[4]) if pd.isna(x.iloc[5]) \n",
    "                                      else x.iloc[5],axis=1)\n",
    "else:\n",
    "    print('No matched employers are found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "345091e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:44:45.795838Z",
     "start_time": "2022-05-18T12:44:45.786560Z"
    }
   },
   "outputs": [],
   "source": [
    "# Update the job postings without industry with \"unclassified\"\n",
    "df_main['industry']=df_main['industry'].apply(lambda x: \"unclassified\" if pd.isna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "dfd9e4c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:44:47.518804Z",
     "start_time": "2022-05-18T12:44:47.506705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify how many jobs'industry names are empty\n",
    "len(df_main[df_main['industry'].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5674b9d",
   "metadata": {},
   "source": [
    "## Extract skills from job desription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a5b5ed63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:46:11.441850Z",
     "start_time": "2022-05-18T12:46:11.417573Z"
    }
   },
   "outputs": [],
   "source": [
    "default_stemmer = PorterStemmer()\n",
    "default_stopwords = stopwords.words('english') # or any other list of your choice\n",
    "def clean_text(text, ):\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        return [w for s in sent_tokenize(text) for w in word_tokenize(s)]\n",
    "\n",
    "    def remove_special_characters(text, characters=string.punctuation.replace('-', '')):\n",
    "        tokens = tokenize_text(text)\n",
    "        pattern = re.compile('[{}]'.format(re.escape(characters)))\n",
    "        return ' '.join(filter(None, [pattern.sub('', t) for t in tokens]))\n",
    "\n",
    "    def stem_text(text, stemmer=default_stemmer):\n",
    "        tokens = tokenize_text(text)\n",
    "        return ' '.join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "    def remove_stopwords(text, stop_words=default_stopwords):\n",
    "        tokens = [w for w in tokenize_text(text) if w not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    text = text.strip(' ') # strip whitespaces\n",
    "    text = text.lower() # lowercase\n",
    "    #text = stem_text(text) # stemming\n",
    "    text = remove_special_characters(text) # remove punctuation and symbols\n",
    "    text = remove_stopwords(text) # remove stopwords\n",
    "    #text.strip(' ') # strip whitespaces again?\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "653ea418",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:46:15.284696Z",
     "start_time": "2022-05-18T12:46:13.053441Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create column \"description_cln\" to store the result of text cleaning\n",
    "df_main['description_cln']=df_main['description'].apply(lambda x: clean_text(x, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7d38ba",
   "metadata": {},
   "source": [
    "### Extract skills from AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f011a14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T10:07:07.073449Z",
     "start_time": "2022-05-18T10:02:29.891169Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 18:02:29,963 - INFO     - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling DetectKeyPhrases\n",
      "End of DetectKeyPhrases\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_frac=1 # % of total records for sample processing.\n",
    "cf_score=0.4  # confidence score threshold for key phrases\n",
    "\n",
    "# Call AWS comprehend to extract key phrases\n",
    "\n",
    "comprehend = boto3.client(service_name='comprehend', region_name='us-east-2')\n",
    "                \n",
    "#text = \"It is raining today in Seattle\"\n",
    "\n",
    "print('Calling DetectKeyPhrases')\n",
    "#df=pd.DataFrame()\n",
    "df_list=[]\n",
    "j=0\n",
    "for i in range(round(sample_frac*len(df_main))):\n",
    "    j=int(len(df_main[\"description_cln\"].iloc[i]) / 5000)+1\n",
    "    txt=[]\n",
    "    for x in range(j): \n",
    "        if j<=1:\n",
    "            txt.append(df_main[\"description_cln\"].iloc[i])\n",
    "        else:\n",
    "            txt.append(df_main[\"description_cln\"].iloc[i][x*5000:(x+1)*5000])\n",
    "        dump_json=json.dumps(comprehend.detect_key_phrases(Text=txt[x]\n",
    "                                                           , LanguageCode='en'), sort_keys=True, indent=4)\n",
    "        df_phrases=pd.json_normalize(json.loads(dump_json)['KeyPhrases'])\n",
    "        df_phrases['id']=df_main[\"id\"].iloc[i]\n",
    "        df_list.append(df_phrases)\n",
    "        \n",
    "df=pd.concat(df_list)\n",
    "print('End of DetectKeyPhrases\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cce344c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T10:07:07.087956Z",
     "start_time": "2022-05-18T10:07:07.076677Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate the dataframe of skills\n",
    "df_skills=pd.DataFrame(df[df['Score']>=cf_score][['id','Text']])\n",
    "df_skills.columns=['id','skill']\n",
    "df_skills['type']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a006b497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:55:34.063962Z",
     "start_time": "2022-05-18T12:55:33.899790Z"
    }
   },
   "outputs": [],
   "source": [
    "# Export skills produced by aws\n",
    "aws_skills= './01_data/output/aws_skills.csv'\n",
    "df_skills.to_csv(aws_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7afa4767",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T10:07:07.226148Z",
     "start_time": "2022-05-18T10:07:07.111805Z"
    }
   },
   "outputs": [],
   "source": [
    "# EDA on aws results\n",
    "d1 = dtale.show(df_skills)\n",
    "d1.open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a679c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T10:21:55.323008Z",
     "start_time": "2022-05-18T10:07:07.234562Z"
    }
   },
   "outputs": [],
   "source": [
    "# Call Azure text analytics to identify name entities\n",
    "cred=input(\"Please input azure's credential\")\n",
    "credential = AzureKeyCredential(cred)\n",
    "endpoint=\"https://topskills.cognitiveservices.azure.com/\"\n",
    "\n",
    "text_analytics_client = TextAnalyticsClient(endpoint, credential)\n",
    "\n",
    "#df_list=[]\n",
    "df_list=pd.DataFrame(columns=['id','skill','category','confidence score'])\n",
    "j=0\n",
    "for i in range(round(sample_frac*len(df_main))):\n",
    "    j=int(len(df_main[\"description\"].iloc[i]) / 5000)+1\n",
    "    txt=[]\n",
    "    for x in range(j): \n",
    "        if j<=1:\n",
    "            txt.append(df_main[\"description\"].iloc[i])\n",
    "        else:\n",
    "            txt.append(df_main[\"description\"].iloc[i][x*5000:(x+1)*5000])\n",
    "    \n",
    "        documents = [item for item in txt[x:x+1]]\n",
    "        #print(documents)\n",
    "        response = text_analytics_client.recognize_entities(documents, language=\"en\")\n",
    "        result = [doc for doc in response if not doc.is_error]\n",
    "        #print(result)\n",
    "        for doc in result:\n",
    "            #print(doc)\n",
    "            for entity in doc.entities:\n",
    "                df_list=df_list.append({'id':df_main['id'].iloc[i],\n",
    "                                         'skill':entity.text,\n",
    "                                        'category':entity.category,\n",
    "                                        'confidence score':entity.confidence_score},ignore_index=True)\n",
    "\n",
    "df_skills_az=df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7da82720",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:55:28.952206Z",
     "start_time": "2022-05-18T12:55:28.721640Z"
    }
   },
   "outputs": [],
   "source": [
    "# Export skills produced by aws\n",
    "az_skills= './01_data/output/az_skills.csv'\n",
    "df_skills_az.to_csv(az_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb75c23a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T10:21:55.508796Z",
     "start_time": "2022-05-18T10:21:55.327451Z"
    }
   },
   "outputs": [],
   "source": [
    "# EDA on azure results\n",
    "d = dtale.show(df_skills_az)\n",
    "d.open_browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c578da1",
   "metadata": {},
   "source": [
    "#### Verify the skill extraction results on both AWS and Azure\n",
    "    1. Randomly choose 10 jobs from glassdoor data scientist'data set.\n",
    "    2. Mannually identify the skills on those job descriptions.\n",
    "    3. Output the skill identification results from AWS and Azure respectively.\n",
    "    4. Compare the manual results with the results of AWS and Azure to choose better one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fa8fbb55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:46:32.411659Z",
     "start_time": "2022-05-18T12:46:32.398523Z"
    }
   },
   "outputs": [],
   "source": [
    "state_num=13\n",
    "samples=10\n",
    "#li_sample=df_main['id'].sample(n=samples, random_state=state_num).to_list()\n",
    "li_sample=[4197200540,4035778057,4163505477,4170026582,4101129848,4147274432,4141919159,4182008825,4121953071\n",
    ",4209136866]\n",
    "df_samples=df_main[df_main['id'].isin(li_sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2dcb16ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:46:33.586712Z",
     "start_time": "2022-05-18T12:46:33.573307Z"
    }
   },
   "outputs": [],
   "source": [
    "# Export samples into csv file for mannual identification of skills\n",
    "sample_file= './01_data/output/samples.csv'\n",
    "df_samples.to_csv(sample_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "583b2c61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:46:34.549326Z",
     "start_time": "2022-05-18T12:46:34.534545Z"
    }
   },
   "outputs": [],
   "source": [
    "df_samples_aw=df_skills[df_skills['id'].isin(li_sample)]\n",
    "df_samples_az=df_skills_az[df_skills_az['id'].isin(li_sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "473e0b67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:46:36.129826Z",
     "start_time": "2022-05-18T12:46:36.118150Z"
    }
   },
   "outputs": [],
   "source": [
    "# Export main table only including the jobs of data scientists\n",
    "aws_file= './01_data/output/sample_aws.csv'\n",
    "df_samples_aw.to_csv(aws_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a9087834",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T12:46:39.550003Z",
     "start_time": "2022-05-18T12:46:39.538351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Export main table only including the jobs of data scientists\n",
    "az_file= './01_data/output/sample_az.csv'\n",
    "df_samples_az.to_csv(az_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf2524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
